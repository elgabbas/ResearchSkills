---
title: 'Research Skills Practical: Regression with R (ca. 1.5h)'
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    keep_md: yes
---

```{r, echo = F}
set.seed(123)
knitr::opts_chunk$set(fig.width=6, fig.height=6)
```


#### Notes for the user

* *This practical tutorial of the [RS course](http://florianhartig.github.io/ResearchSkills/) is intended to be read together with the more detailed [Statstics lecture notes](https://www.dropbox.com/s/s38ge7pjgf55qs1/EssentialStatistics.pdf?dl=0)*

* *If you had problems to follow the practical, see links to websites with further explanations at the end of this document!*

* *To run the code demonstrated here, you should install R and RStudio*


## Continous response variables

### Linear regression

Linear regression is the simples form of regression. We can use this when the response is continous. The assumption is that the response depends on the predictors as in 

y ~ par1 * pred1 +  par2 * pred2 +  par3 * pred2^2 + ... + residual Error

where the parameters par1 ... par3 are estimated, and the residual error is normally distributed. 

Let's look at some example, using the data from Monday. We see that there is a correlation between Ozone and Temperature. 

```{r}
plot(airquality$Temp~airquality$Ozone)
```

With the lm() command, we can ask R to try to get the best fitting straight line between the two variables. 

```{r}
fit = lm(airquality$Temp~airquality$Ozone)
```

Let's look at the result visually first

```{r}
plot(airquality$Ozone, airquality$Temp)
abline(fit, col = "blue")
```

Here's the detailed output

```{r}
summary(fit)
```

In the output, we see the parameters for the effect of Ozone (called the regression slope), and the intercept. 

R knows the line is straight because we tell the program that airquality$Temp~airquality$Ozone . We'll see later how to modify this if we want to fit other functions. 

#### Residual analysis

with plot(fit), we get the residuals (the deviation from the straight line). As said, the linear regression assumes that those are normally distributed, so we should check whether this is really the case. 

```{r}
par(mfrow=c(2,2))
plot(fit)
```

Here, the residuals are not really homogenously scattering around the predicted value, suggesting that the model doesn't fit very well. Well, one could have already guessed this, because the correlation doesn't look very linear. We can add a quadratic term by 


```{r}
fit2 = lm(airquality$Temp~airquality$Ozone + I(airquality$Ozone^2))
summary(fit2)
```

Residuals look better now

```{r}
par(mfrow=c(2,2))
plot(fit2)
```

Plotting the results

```{r}
plot(airquality$Ozone, airquality$Temp)
points(fit2$model[,2], predict(fit2), col = "blue")
```

### Linear regression with categorical predictors 

If we have categorical varialbles such as in this dataset

```{r}
boxplot(PlantGrowth$weight~PlantGrowth$group, main = "growth of plants")
```

This still works:

```{r}
fit <- lm(weight~group, data = PlantGrowth)
summary(fit)
```

A thing that is confusing many people now, however, is that we have two parameter estimates for the one predictor variable. The reson is that there are 3 groups in the dataset. The first group is set in this case automatically as reference, and for the other groups, predictors are estimated. The p-values for those predictors are thus against the reference (if I take predictor trt1 out, it will get the value of ctrl). Hence, these p-values for categorical variables depend on the order of the varialbles (you could reorder to have trt1 as reference.)


#### ANOVA

A question that often pops up in this context is: is there are difference between the groups at all? The regression only shows us whether there is a signficant difference between the first factor and the two others. If we want to test for overall differences, we can make an ANOVA of the fitted object

```{r}
aovresult <- aov(fit)
summary(aovresult)
```

Note that we get now significance for an overall difference, although we didn't have significance in the regression before. 

We can now use so-called post-hoc tests to find out which differences are significant ()

See more examples, with more factors here http://www.statmethods.net/stats/anova.html

Note: aov is designed for balanced designs, and the results can be hard to interpret without balance: beware that missing values in the response(s) will likely lose the balance. If there are two or more error strata, the methods used are statistically inefficient without balance, and it may be better to use lme in package nlme.

#### T-test

A simple option if you just want to test two groups against each other is the t-test. It assumes normal distribution within the groups. We can use this to do post-hoc testing for the example above: 

```{r}
attach(PlantGrowth)
t.test(weight[group=='ctrl'], weight[group=="trt1"])
```

testing against group 2

```{r}
t.test(weight[group=='ctrl'], weight[group=="trt2"])
detach(PlantGrowth)
```

but if we really would have done both tests, we would have had to correct for multiple testing

```{r}
p.adjust(c(0.2504, 0.0479), method = "holm")
  ```

How does this work? Write ?p.adjust in the console. Also, read http://webdev.cas.msu.edu/cas992/weeks/week10.html

After correction, the values wouldn't be significant any more 

## Discrete response variables 

### The generalized linear model framework

The glm is a generalization of lm. We could create a model identical to lm() by glm(formula, family = gaussian(link = "identity")), but the advantage is that glm has more options, among them the following defaults

binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")

but step for step ... let's look at an example for binomial data


### 0/1 Response - the logistic regression

```{r}
library(effects) 
data(TitanicSurvival)
head(TitanicSurvival)
str(TitanicSurvival)
attach(TitanicSurvival)
```

Let's visualize this. About visualizing associations see http://www.statmethods.net/advgraphs/mosaic.html

We'll use the mosaic plot. May be you have to install.packages("vcd")

```{r}
library(vcd)
mosaic(~ sex + passengerClass + survived, shade=TRUE, legend=TRUE) 
surv <- as.numeric(survived)-1 # glm requires 0 / 1 not true false
```

How do we analyze this data? Response clearly not normal, but 1/0. now, the glm is basically the same as lm(), just that you specify the family.

Let's first test whether survival is correlated to age

```{r}
fmt <- glm(surv ~ age, family=binomial)
summary(fmt)
```

Result plotted 

```{r}
plot(surv ~ age, main="only age term")
newage <- seq(min(age, na.rm=T), max(age, na.rm=T), len=100)
preds <- predict(fmt, newdata=data.frame("age"=newage), se.fit=T)
lines(newage, plogis(preds$fit), col="purple", lwd=3)
lines(newage, plogis(preds$fit-2*preds$se.fit), col="purple", lwd=3, lty=2)
lines(newage, plogis(preds$fit+2*preds$se.fit), col="purple", lwd=3, lty=2)
```

Now, let's out all relevant variables in 

```{r}
surv <- as.numeric(survived)-1 # glm requires 0 / 1 not true false
fmt <- glm(surv ~ age  + sex + passengerClass, family=binomial)
summary(fmt)
```

#### ANOVA for GLM

if you want an ANOVA

```{r}
library(car)
Anova(fmt)

detach(TitanicSurvival)
```

### Count data - Poisson Regression

For count data, we use the glm with the Poisson error distribution. Here's some observations of the pieces of food given to young birds, and their perceived attractiveness.

```{r}
cfc <- data.frame(
  stuecke = c(3,6,8,4,2,7,6,8,10,3,5,7,6,7,5,6,7,11,8,11,13,11,7,7,6),
  attrakt = c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5) 
)
attach(cfc)
plot(stuecke ~ attrakt)
```

This is how you specify the poisson

```{r}
fm <- glm(stuecke ~ attrakt, family=poisson)
summary(fm)
```

predictions

```{r}
newattrakt <- c(1,1.5,2,2.5,3,3.5,4,4.5,5)
preds <- predict(fm, newdata=data.frame("attrakt"=newattrakt))
plot(stuecke ~ attrakt)
lines(newattrakt, exp(preds), lwd=2, col="green")
```

same with 95% confidence interval:

```{r}
preds <- predict(fm, newdata=data.frame("attrakt"=newattrakt), se.fit=T)
str(preds)
plot(stuecke ~ attrakt)
lines(newattrakt, exp(preds$fit), lwd=2, col="green")
lines(newattrakt, exp(preds$fit+2*preds$se.fit), lwd=2, col="green", lty=2)
lines(newattrakt, exp(preds$fit-2*preds$se.fit), lwd=2, col="green", lty=2)

detach(cfc)
```



### Multinomial Data - multinomial regression

If you have several options for the response (red, green, blue), you are fitting a multinomial regression. This is not in the standard glm package. The standard package to do this would be mlogit. I give an example below. The problem with mlogit is that it requires data in a particular way, i.e. that for every observation, every choice is a single line, and then there is one column that tells you which choice was made (yes / no). To use mlogit, you need to reshape you data to this format. 

If you don't have your data in this format, and don't want to reshape, an alternative is the less powerful but easier to use multinom function from the nnet package. An example of how to use this function is [here](http://www.ats.ucla.edu/stat/stata/dae/mlogit.htm)


#### mlogit example


```{r}
library(mlogit)
 
data("Fishing", package = "mlogit")
head(Fishing)
```
The data we are using is a dataframe containing :

mode
recreation mode choice, one of : beach, pier, boat and charter

price.beach
price for beach mode

price.pier
price for pier mode

price.boat
price for private boat mode

price.charter
price for charter boat mode

catch.beach
catch rate for beach mode

catch.pier
catch rate for pier mode

catch.boat
catch rate for private boat mode

catch.charter
catch rate for charter boat mode

income
monthly income

We transform this in an object mlogit can work with by

```{r}
Fish <- mlogit.data(Fishing, varying = c(2:9), shape = "wide", choice = "mode")
```
varying tells the model that those are variables that are specific to the alternative outcomes of the response, i.e. price of the boat, while variables that are not varying are independent of the outcome, e.g. income. 

```{r}

## a pure "conditional" model
 
summary(mlogit(mode ~ price + catch, data = Fish))

```
Fits, for each outcome, the increase of the choice with the price and catch. Hence, we get the intercepts for each outcome, and one parameter fitted per variable that is the effect of price / choice on the probability to choose any of the 4 outcomes. 

If we correlate the variable income, this is different. Income is not specific to the choice (not selected as varying when we set up the data). In this case, we ask how the choice differs when we have people of different incomes. Hence, we obtain an intervept value per parameter, and an estimate for whether this choice is affected by an increase of income, relative to the baseline (pier).

```{r}
## a pure "multinomial model"
 
summary(mlogit(mode ~ 0 | income, data = Fish))
```

Note that if you want to interpret the parameter values, you have to transform back to the response with the relatively complicated link function of the multinomial logistic regression, see formula here http://en.wikipedia.org/wiki/Multinomial_logistic_regression 

See more examples at http://www.inside-r.org/packages/cran/mlogit/docs/suml

And this is a good tutorial http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf


---
**Copyright, reuse and updates**: By Florian Hartig. Updates will be posted at http://florianhartig.github.io/ResearchSkills/. Reuse permitted under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Licens


